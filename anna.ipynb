{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt','r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c:i for i,c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr,n_seqs,n_steps):\n",
    "    '''\n",
    "    对已有的数组进行mini-batch分割\n",
    "    \n",
    "    arr：待分割的数组\n",
    "    n_seqs:一个batch中序列个数\n",
    "    n_steps：单个序列包含的字符数目\n",
    "    '''\n",
    "    batch_size = n_seqs*n_steps\n",
    "    n_batches = int(len(arr)/batch_size)\n",
    "    print('n_batches:',n_batches)\n",
    "    #这里我们仅保留完整的batch，对于不能整除的部分进行舍弃\n",
    "    arr = arr[:int(batch_size * n_batches)]\n",
    "    \n",
    "    #reconstruct\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0,arr.shape[1],n_steps):\n",
    "        #inputs\n",
    "        x = arr[:,n:n+n_steps]\n",
    "        #targets\n",
    "        y = np.zeros_like(x)\n",
    "        y[:,:-1],y[:,-1] = x[:,1:],x[:,0]\n",
    "        yield x,y\n",
    "    #上面的代码定义了一个generator，调用函数会返回一个generator对象，我们可以获取一个batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3970\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded,10,50)\n",
    "x,y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[45 61 47 27 19  6 28 77 20 72]\n",
      " [77 47 53 77 39 43 19 77 41 43]\n",
      " [81  1 39  8 72 72 69 23  6 50]\n",
      " [39 77 82 31 28  1 39 41 77 61]\n",
      " [77  1 19 77  1 50 62 77 50  1]\n",
      " [77 22 19 77 16 47 50 72 43 39]\n",
      " [61  6 39 77 10 43 53  6 77 15]\n",
      " [ 3 77  9 31 19 77 39 43 16 77]\n",
      " [19 77  1 50 39 56 19  8 77 36]\n",
      " [77 50 47  1 82 77 19 43 77 61]]\n",
      "\n",
      "y\n",
      " [[61 47 27 19  6 28 77 20 72 72]\n",
      " [47 53 77 39 43 19 77 41 43  1]\n",
      " [ 1 39  8 72 72 69 23  6 50 62]\n",
      " [77 82 31 28  1 39 41 77 61  1]\n",
      " [ 1 19 77  1 50 62 77 50  1 28]\n",
      " [22 19 77 16 47 50 72 43 39  2]\n",
      " [ 6 39 77 10 43 53  6 77 15 43]\n",
      " [77  9 31 19 77 39 43 16 77 50]\n",
      " [77  1 50 39 56 19  8 77 36 61]\n",
      " [50 47  1 82 77 19 43 77 61  6]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n',x[:10,:10])\n",
    "print('\\ny\\n',y[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(num_seqs,num_steps):\n",
    "    '''\n",
    "    构建输入层\n",
    "    num_seqs:每个batch中序列的个数\n",
    "    num_steps:每个序列包含的字符数\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='inputs')\n",
    "    targets = tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='targets')\n",
    "    \n",
    "    #add keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM层\n",
    "def build_lstm(lstm_size,num_layers,batch_size,keep_prob):\n",
    "    '''\n",
    "    构建lstm层\n",
    "    keep_prob\n",
    "    lstm_size:lstm隐层中结点数目\n",
    "    num_layers:lstm的隐层数目\n",
    "    batch_size:batch_size\n",
    "    '''\n",
    "    #构建一个基本lstm单元\n",
    "    lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    #add dropout\n",
    "    drop = tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "    \n",
    "    #stack\n",
    "    \n",
    "    '''\n",
    "    stacked_rnn = []\n",
    "    for iiLyr in range(3):\n",
    "        stacked_rnn.append(tf.nn.rnn_cell.LSTMCell(num_units=512, state_is_tuple=True))\n",
    "    MultiLyr_cell = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn, state_is_tuple=True)\n",
    "    '''\n",
    "    \n",
    "    #cell = tf.nn.rnn_cell.MultiRNNCell([drop for _ in range(num_layers)])#这里就是把drop层复制了num_layers层\n",
    "    #initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    stacked_rnn = []\n",
    "    for iiLyr in range(num_layers):\n",
    "        lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "        stacked_rnn.append(drop)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn,state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "\n",
    "    return cell,initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output layer\n",
    "def build_output(lstm_output,in_size,out_size):\n",
    "    '''\n",
    "    构造输出层\n",
    "    \n",
    "    lstm_output:lstm层输出的结果 (是一个三维数组)\n",
    "    in_size: lstm输出层重塑后的size\n",
    "    out_size：softmax层的size\n",
    "    '''\n",
    "    \n",
    "    #将lstm的输出按照列concate，例如[[1,2,3],[7,8,9]],\n",
    "    #tf.concat的结果是[1,2,3,7,8,9]\n",
    "    seq_output = tf.concat(lstm_output, axis=1) #tf.concat(concat_dim,values)\n",
    "    #reshape\n",
    "    x = tf.reshape(seq_output,[-1,in_size])\n",
    "    \n",
    "    #将lstm层与softmax层全链接\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size,out_size],stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # calculate logits\n",
    "    logits = tf.matmul(x,softmax_w) + softmax_b\n",
    "    \n",
    "    #softmax层返回概率分布\n",
    "    out = tf.nn.softmax(logits,name='predictions')\n",
    "    \n",
    "    return out,logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练误差计算\n",
    "def build_loss(logits,targets,lstm_size,num_classes):\n",
    "    '''\n",
    "    根据logits和targets计算损失\n",
    "    \n",
    "    logits:全链接层的输出结果\n",
    "    targets:targets\n",
    "    lstm_size\n",
    "    num_classes: vocab_size\n",
    "    '''\n",
    "    \n",
    "    #one_hot encode\n",
    "    y_one_hot = tf.one_hot(targets,num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot,logits.get_shape())\n",
    "    \n",
    "    #softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss,learning_rate,grad_clip):\n",
    "    '''\n",
    "    构造Optimizer\n",
    "    \n",
    "    loss:损失\n",
    "    learning_rate:学习率\n",
    "    '''\n",
    "    \n",
    "    #use clipping gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads,_ = tf.clip_by_global_norm(tf.gradients(loss,tvars),grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads,tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self,num_classes,batch_size=64,num_steps=50,\n",
    "                lstm_size=128,num_layers=2,learning_rate=0.001,\n",
    "                grad_clip=5,sampling=False):\n",
    "        \n",
    "        if sampling == True:\n",
    "            batch_size,num_steps = 1,1\n",
    "        else:\n",
    "            batch_size,num_steps = batch_size,num_steps\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #input layer\n",
    "        self.inputs,self.targets,self.keep_prob = build_inputs(batch_size,num_steps)\n",
    "        \n",
    "        #lstm layer\n",
    "        cell,self.initial_state = build_lstm(lstm_size,num_layers,batch_size,self.keep_prob)\n",
    "        \n",
    "        # 对输入进行one_hot编码\n",
    "        x_one_hot = tf.one_hot(self.inputs,num_classes)\n",
    "        \n",
    "        # run rnn\n",
    "        #outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # predict result\n",
    "        self.prediction, self.logits = build_output(outputs,lstm_size,num_classes)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss,learning_rate,grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate =0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "epoch: 0 \n",
      " counter: 0\n",
      "epoch: 0 \n",
      " counter: 1\n",
      "epoch: 0 \n",
      " counter: 2\n",
      "epoch: 0 \n",
      " counter: 3\n",
      "epoch: 0 \n",
      " counter: 4\n",
      "epoch: 0 \n",
      " counter: 5\n",
      "epoch: 0 \n",
      " counter: 6\n",
      "epoch: 0 \n",
      " counter: 7\n",
      "epoch: 0 \n",
      " counter: 8\n",
      "epoch: 0 \n",
      " counter: 9\n",
      "epoch: 0 \n",
      " counter: 10\n",
      "epoch: 0 \n",
      " counter: 11\n",
      "epoch: 0 \n",
      " counter: 12\n",
      "epoch: 0 \n",
      " counter: 13\n",
      "epoch: 0 \n",
      " counter: 14\n",
      "epoch: 0 \n",
      " counter: 15\n",
      "epoch: 0 \n",
      " counter: 16\n",
      "epoch: 0 \n",
      " counter: 17\n",
      "epoch: 0 \n",
      " counter: 18\n",
      "epoch: 0 \n",
      " counter: 19\n",
      "epoch: 0 \n",
      " counter: 20\n",
      "epoch: 0 \n",
      " counter: 21\n",
      "epoch: 0 \n",
      " counter: 22\n",
      "epoch: 0 \n",
      " counter: 23\n",
      "epoch: 0 \n",
      " counter: 24\n",
      "epoch: 0 \n",
      " counter: 25\n",
      "epoch: 0 \n",
      " counter: 26\n",
      "epoch: 0 \n",
      " counter: 27\n",
      "epoch: 0 \n",
      " counter: 28\n",
      "epoch: 0 \n",
      " counter: 29\n",
      "epoch: 0 \n",
      " counter: 30\n",
      "epoch: 0 \n",
      " counter: 31\n",
      "epoch: 0 \n",
      " counter: 32\n",
      "epoch: 0 \n",
      " counter: 33\n",
      "epoch: 0 \n",
      " counter: 34\n",
      "epoch: 0 \n",
      " counter: 35\n",
      "epoch: 0 \n",
      " counter: 36\n",
      "epoch: 0 \n",
      " counter: 37\n",
      "epoch: 0 \n",
      " counter: 38\n",
      "epoch: 0 \n",
      " counter: 39\n",
      "epoch: 0 \n",
      " counter: 40\n",
      "epoch: 0 \n",
      " counter: 41\n",
      "epoch: 0 \n",
      " counter: 42\n",
      "epoch: 0 \n",
      " counter: 43\n",
      "epoch: 0 \n",
      " counter: 44\n",
      "epoch: 0 \n",
      " counter: 45\n",
      "epoch: 0 \n",
      " counter: 46\n",
      "epoch: 0 \n",
      " counter: 47\n",
      "epoch: 0 \n",
      " counter: 48\n",
      "epoch: 0 \n",
      " counter: 49\n",
      "epoch: 0 \n",
      " counter: 50\n",
      "epoch: 0 \n",
      " counter: 51\n",
      "epoch: 0 \n",
      " counter: 52\n",
      "epoch: 0 \n",
      " counter: 53\n",
      "epoch: 0 \n",
      " counter: 54\n",
      "epoch: 0 \n",
      " counter: 55\n",
      "epoch: 0 \n",
      " counter: 56\n",
      "epoch: 0 \n",
      " counter: 57\n",
      "epoch: 0 \n",
      " counter: 58\n",
      "epoch: 0 \n",
      " counter: 59\n",
      "epoch: 0 \n",
      " counter: 60\n",
      "epoch: 0 \n",
      " counter: 61\n",
      "epoch: 0 \n",
      " counter: 62\n",
      "epoch: 0 \n",
      " counter: 63\n",
      "epoch: 0 \n",
      " counter: 64\n",
      "epoch: 0 \n",
      " counter: 65\n",
      "epoch: 0 \n",
      " counter: 66\n",
      "epoch: 0 \n",
      " counter: 67\n",
      "epoch: 0 \n",
      " counter: 68\n",
      "epoch: 0 \n",
      " counter: 69\n",
      "epoch: 0 \n",
      " counter: 70\n",
      "epoch: 0 \n",
      " counter: 71\n",
      "epoch: 0 \n",
      " counter: 72\n",
      "epoch: 0 \n",
      " counter: 73\n",
      "epoch: 0 \n",
      " counter: 74\n",
      "epoch: 0 \n",
      " counter: 75\n",
      "epoch: 0 \n",
      " counter: 76\n",
      "epoch: 0 \n",
      " counter: 77\n",
      "epoch: 0 \n",
      " counter: 78\n",
      "epoch: 0 \n",
      " counter: 79\n",
      "epoch: 0 \n",
      " counter: 80\n",
      "epoch: 0 \n",
      " counter: 81\n",
      "epoch: 0 \n",
      " counter: 82\n",
      "epoch: 0 \n",
      " counter: 83\n",
      "epoch: 0 \n",
      " counter: 84\n",
      "epoch: 0 \n",
      " counter: 85\n",
      "epoch: 0 \n",
      " counter: 86\n",
      "epoch: 0 \n",
      " counter: 87\n",
      "epoch: 0 \n",
      " counter: 88\n",
      "epoch: 0 \n",
      " counter: 89\n",
      "epoch: 0 \n",
      " counter: 90\n",
      "epoch: 0 \n",
      " counter: 91\n",
      "epoch: 0 \n",
      " counter: 92\n",
      "epoch: 0 \n",
      " counter: 93\n",
      "epoch: 0 \n",
      " counter: 94\n",
      "epoch: 0 \n",
      " counter: 95\n",
      "epoch: 0 \n",
      " counter: 96\n",
      "epoch: 0 \n",
      " counter: 97\n",
      "epoch: 0 \n",
      " counter: 98\n",
      "epoch: 0 \n",
      " counter: 99\n",
      "轮数: 1/2...  训练步数: 100...  训练误差: 3.0955...  5.8149 sec/batch\n",
      "epoch: 0 \n",
      " counter: 100\n",
      "epoch: 0 \n",
      " counter: 101\n",
      "epoch: 0 \n",
      " counter: 102\n",
      "epoch: 0 \n",
      " counter: 103\n",
      "epoch: 0 \n",
      " counter: 104\n",
      "epoch: 0 \n",
      " counter: 105\n",
      "epoch: 0 \n",
      " counter: 106\n",
      "epoch: 0 \n",
      " counter: 107\n",
      "epoch: 0 \n",
      " counter: 108\n",
      "epoch: 0 \n",
      " counter: 109\n",
      "epoch: 0 \n",
      " counter: 110\n",
      "epoch: 0 \n",
      " counter: 111\n",
      "epoch: 0 \n",
      " counter: 112\n",
      "epoch: 0 \n",
      " counter: 113\n",
      "epoch: 0 \n",
      " counter: 114\n",
      "epoch: 0 \n",
      " counter: 115\n",
      "epoch: 0 \n",
      " counter: 116\n",
      "epoch: 0 \n",
      " counter: 117\n",
      "epoch: 0 \n",
      " counter: 118\n",
      "epoch: 0 \n",
      " counter: 119\n",
      "epoch: 0 \n",
      " counter: 120\n",
      "epoch: 0 \n",
      " counter: 121\n",
      "epoch: 0 \n",
      " counter: 122\n",
      "epoch: 0 \n",
      " counter: 123\n",
      "epoch: 0 \n",
      " counter: 124\n",
      "epoch: 0 \n",
      " counter: 125\n",
      "epoch: 0 \n",
      " counter: 126\n",
      "epoch: 0 \n",
      " counter: 127\n",
      "epoch: 0 \n",
      " counter: 128\n",
      "epoch: 0 \n",
      " counter: 129\n",
      "epoch: 0 \n",
      " counter: 130\n",
      "epoch: 0 \n",
      " counter: 131\n",
      "epoch: 0 \n",
      " counter: 132\n",
      "epoch: 0 \n",
      " counter: 133\n",
      "epoch: 0 \n",
      " counter: 134\n",
      "epoch: 0 \n",
      " counter: 135\n",
      "epoch: 0 \n",
      " counter: 136\n",
      "epoch: 0 \n",
      " counter: 137\n",
      "epoch: 0 \n",
      " counter: 138\n",
      "epoch: 0 \n",
      " counter: 139\n",
      "epoch: 0 \n",
      " counter: 140\n",
      "epoch: 0 \n",
      " counter: 141\n",
      "epoch: 0 \n",
      " counter: 142\n",
      "epoch: 0 \n",
      " counter: 143\n",
      "epoch: 0 \n",
      " counter: 144\n",
      "epoch: 0 \n",
      " counter: 145\n",
      "epoch: 0 \n",
      " counter: 146\n",
      "epoch: 0 \n",
      " counter: 147\n",
      "epoch: 0 \n",
      " counter: 148\n",
      "epoch: 0 \n",
      " counter: 149\n",
      "epoch: 0 \n",
      " counter: 150\n",
      "epoch: 0 \n",
      " counter: 151\n",
      "epoch: 0 \n",
      " counter: 152\n",
      "epoch: 0 \n",
      " counter: 153\n",
      "epoch: 0 \n",
      " counter: 154\n",
      "epoch: 0 \n",
      " counter: 155\n",
      "epoch: 0 \n",
      " counter: 156\n",
      "epoch: 0 \n",
      " counter: 157\n",
      "epoch: 0 \n",
      " counter: 158\n",
      "epoch: 0 \n",
      " counter: 159\n",
      "epoch: 0 \n",
      " counter: 160\n",
      "epoch: 0 \n",
      " counter: 161\n",
      "epoch: 0 \n",
      " counter: 162\n",
      "epoch: 0 \n",
      " counter: 163\n",
      "epoch: 0 \n",
      " counter: 164\n",
      "epoch: 0 \n",
      " counter: 165\n",
      "epoch: 0 \n",
      " counter: 166\n",
      "epoch: 0 \n",
      " counter: 167\n",
      "epoch: 0 \n",
      " counter: 168\n",
      "epoch: 0 \n",
      " counter: 169\n",
      "epoch: 0 \n",
      " counter: 170\n",
      "epoch: 0 \n",
      " counter: 171\n",
      "epoch: 0 \n",
      " counter: 172\n",
      "epoch: 0 \n",
      " counter: 173\n",
      "epoch: 0 \n",
      " counter: 174\n",
      "epoch: 0 \n",
      " counter: 175\n",
      "epoch: 0 \n",
      " counter: 176\n",
      "epoch: 0 \n",
      " counter: 177\n",
      "epoch: 0 \n",
      " counter: 178\n",
      "epoch: 0 \n",
      " counter: 179\n",
      "epoch: 0 \n",
      " counter: 180\n",
      "epoch: 0 \n",
      " counter: 181\n",
      "epoch: 0 \n",
      " counter: 182\n",
      "epoch: 0 \n",
      " counter: 183\n",
      "epoch: 0 \n",
      " counter: 184\n",
      "epoch: 0 \n",
      " counter: 185\n",
      "epoch: 0 \n",
      " counter: 186\n",
      "epoch: 0 \n",
      " counter: 187\n",
      "epoch: 0 \n",
      " counter: 188\n",
      "epoch: 0 \n",
      " counter: 189\n",
      "epoch: 0 \n",
      " counter: 190\n",
      "epoch: 0 \n",
      " counter: 191\n",
      "epoch: 0 \n",
      " counter: 192\n",
      "epoch: 0 \n",
      " counter: 193\n",
      "epoch: 0 \n",
      " counter: 194\n",
      "epoch: 0 \n",
      " counter: 195\n",
      "epoch: 0 \n",
      " counter: 196\n",
      "epoch: 0 \n",
      " counter: 197\n",
      "198\n",
      "epoch: 1 \n",
      " counter: 198\n",
      "epoch: 1 \n",
      " counter: 199\n",
      "轮数: 2/2...  训练步数: 200...  训练误差: 2.4854...  6.1471 sec/batch\n",
      "epoch: 1 \n",
      " counter: 200\n",
      "epoch: 1 \n",
      " counter: 201\n",
      "epoch: 1 \n",
      " counter: 202\n",
      "epoch: 1 \n",
      " counter: 203\n",
      "epoch: 1 \n",
      " counter: 204\n",
      "epoch: 1 \n",
      " counter: 205\n",
      "epoch: 1 \n",
      " counter: 206\n",
      "epoch: 1 \n",
      " counter: 207\n",
      "epoch: 1 \n",
      " counter: 208\n",
      "epoch: 1 \n",
      " counter: 209\n",
      "epoch: 1 \n",
      " counter: 210\n",
      "epoch: 1 \n",
      " counter: 211\n",
      "epoch: 1 \n",
      " counter: 212\n",
      "epoch: 1 \n",
      " counter: 213\n",
      "epoch: 1 \n",
      " counter: 214\n",
      "epoch: 1 \n",
      " counter: 215\n",
      "epoch: 1 \n",
      " counter: 216\n",
      "epoch: 1 \n",
      " counter: 217\n",
      "epoch: 1 \n",
      " counter: 218\n",
      "epoch: 1 \n",
      " counter: 219\n",
      "epoch: 1 \n",
      " counter: 220\n",
      "epoch: 1 \n",
      " counter: 221\n",
      "epoch: 1 \n",
      " counter: 222\n",
      "epoch: 1 \n",
      " counter: 223\n",
      "epoch: 1 \n",
      " counter: 224\n",
      "epoch: 1 \n",
      " counter: 225\n",
      "epoch: 1 \n",
      " counter: 226\n",
      "epoch: 1 \n",
      " counter: 227\n",
      "epoch: 1 \n",
      " counter: 228\n",
      "epoch: 1 \n",
      " counter: 229\n",
      "epoch: 1 \n",
      " counter: 230\n",
      "epoch: 1 \n",
      " counter: 231\n",
      "epoch: 1 \n",
      " counter: 232\n",
      "epoch: 1 \n",
      " counter: 233\n",
      "epoch: 1 \n",
      " counter: 234\n",
      "epoch: 1 \n",
      " counter: 235\n",
      "epoch: 1 \n",
      " counter: 236\n",
      "epoch: 1 \n",
      " counter: 237\n",
      "epoch: 1 \n",
      " counter: 238\n",
      "epoch: 1 \n",
      " counter: 239\n",
      "epoch: 1 \n",
      " counter: 240\n",
      "epoch: 1 \n",
      " counter: 241\n",
      "epoch: 1 \n",
      " counter: 242\n",
      "epoch: 1 \n",
      " counter: 243\n",
      "epoch: 1 \n",
      " counter: 244\n",
      "epoch: 1 \n",
      " counter: 245\n",
      "epoch: 1 \n",
      " counter: 246\n",
      "epoch: 1 \n",
      " counter: 247\n",
      "epoch: 1 \n",
      " counter: 248\n",
      "epoch: 1 \n",
      " counter: 249\n",
      "epoch: 1 \n",
      " counter: 250\n",
      "epoch: 1 \n",
      " counter: 251\n",
      "epoch: 1 \n",
      " counter: 252\n",
      "epoch: 1 \n",
      " counter: 253\n",
      "epoch: 1 \n",
      " counter: 254\n",
      "epoch: 1 \n",
      " counter: 255\n",
      "epoch: 1 \n",
      " counter: 256\n",
      "epoch: 1 \n",
      " counter: 257\n",
      "epoch: 1 \n",
      " counter: 258\n",
      "epoch: 1 \n",
      " counter: 259\n",
      "epoch: 1 \n",
      " counter: 260\n",
      "epoch: 1 \n",
      " counter: 261\n",
      "epoch: 1 \n",
      " counter: 262\n",
      "epoch: 1 \n",
      " counter: 263\n",
      "epoch: 1 \n",
      " counter: 264\n",
      "epoch: 1 \n",
      " counter: 265\n",
      "epoch: 1 \n",
      " counter: 266\n",
      "epoch: 1 \n",
      " counter: 267\n",
      "epoch: 1 \n",
      " counter: 268\n",
      "epoch: 1 \n",
      " counter: 269\n",
      "epoch: 1 \n",
      " counter: 270\n",
      "epoch: 1 \n",
      " counter: 271\n",
      "epoch: 1 \n",
      " counter: 272\n",
      "epoch: 1 \n",
      " counter: 273\n",
      "epoch: 1 \n",
      " counter: 274\n",
      "epoch: 1 \n",
      " counter: 275\n",
      "epoch: 1 \n",
      " counter: 276\n",
      "epoch: 1 \n",
      " counter: 277\n",
      "epoch: 1 \n",
      " counter: 278\n",
      "epoch: 1 \n",
      " counter: 279\n",
      "epoch: 1 \n",
      " counter: 280\n",
      "epoch: 1 \n",
      " counter: 281\n",
      "epoch: 1 \n",
      " counter: 282\n",
      "epoch: 1 \n",
      " counter: 283\n",
      "epoch: 1 \n",
      " counter: 284\n",
      "epoch: 1 \n",
      " counter: 285\n",
      "epoch: 1 \n",
      " counter: 286\n",
      "epoch: 1 \n",
      " counter: 287\n",
      "epoch: 1 \n",
      " counter: 288\n",
      "epoch: 1 \n",
      " counter: 289\n",
      "epoch: 1 \n",
      " counter: 290\n",
      "epoch: 1 \n",
      " counter: 291\n",
      "epoch: 1 \n",
      " counter: 292\n",
      "epoch: 1 \n",
      " counter: 293\n",
      "epoch: 1 \n",
      " counter: 294\n",
      "epoch: 1 \n",
      " counter: 295\n",
      "epoch: 1 \n",
      " counter: 296\n",
      "epoch: 1 \n",
      " counter: 297\n",
      "epoch: 1 \n",
      " counter: 298\n",
      "epoch: 1 \n",
      " counter: 299\n",
      "轮数: 2/2...  训练步数: 300...  训练误差: 2.2546...  6.8389 sec/batch\n",
      "epoch: 1 \n",
      " counter: 300\n",
      "epoch: 1 \n",
      " counter: 301\n",
      "epoch: 1 \n",
      " counter: 302\n",
      "epoch: 1 \n",
      " counter: 303\n",
      "epoch: 1 \n",
      " counter: 304\n",
      "epoch: 1 \n",
      " counter: 305\n",
      "epoch: 1 \n",
      " counter: 306\n",
      "epoch: 1 \n",
      " counter: 307\n",
      "epoch: 1 \n",
      " counter: 308\n",
      "epoch: 1 \n",
      " counter: 309\n",
      "epoch: 1 \n",
      " counter: 310\n",
      "epoch: 1 \n",
      " counter: 311\n",
      "epoch: 1 \n",
      " counter: 312\n",
      "epoch: 1 \n",
      " counter: 313\n",
      "epoch: 1 \n",
      " counter: 314\n",
      "epoch: 1 \n",
      " counter: 315\n",
      "epoch: 1 \n",
      " counter: 316\n",
      "epoch: 1 \n",
      " counter: 317\n",
      "epoch: 1 \n",
      " counter: 318\n",
      "epoch: 1 \n",
      " counter: 319\n",
      "epoch: 1 \n",
      " counter: 320\n",
      "epoch: 1 \n",
      " counter: 321\n",
      "epoch: 1 \n",
      " counter: 322\n",
      "epoch: 1 \n",
      " counter: 323\n",
      "epoch: 1 \n",
      " counter: 324\n",
      "epoch: 1 \n",
      " counter: 325\n",
      "epoch: 1 \n",
      " counter: 326\n",
      "epoch: 1 \n",
      " counter: 327\n",
      "epoch: 1 \n",
      " counter: 328\n",
      "epoch: 1 \n",
      " counter: 329\n",
      "epoch: 1 \n",
      " counter: 330\n",
      "epoch: 1 \n",
      " counter: 331\n",
      "epoch: 1 \n",
      " counter: 332\n",
      "epoch: 1 \n",
      " counter: 333\n",
      "epoch: 1 \n",
      " counter: 334\n",
      "epoch: 1 \n",
      " counter: 335\n",
      "epoch: 1 \n",
      " counter: 336\n",
      "epoch: 1 \n",
      " counter: 337\n",
      "epoch: 1 \n",
      " counter: 338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \n",
      " counter: 339\n",
      "epoch: 1 \n",
      " counter: 340\n",
      "epoch: 1 \n",
      " counter: 341\n",
      "epoch: 1 \n",
      " counter: 342\n",
      "epoch: 1 \n",
      " counter: 343\n",
      "epoch: 1 \n",
      " counter: 344\n",
      "epoch: 1 \n",
      " counter: 345\n",
      "epoch: 1 \n",
      " counter: 346\n",
      "epoch: 1 \n",
      " counter: 347\n",
      "epoch: 1 \n",
      " counter: 348\n",
      "epoch: 1 \n",
      " counter: 349\n",
      "epoch: 1 \n",
      " counter: 350\n",
      "epoch: 1 \n",
      " counter: 351\n",
      "epoch: 1 \n",
      " counter: 352\n",
      "epoch: 1 \n",
      " counter: 353\n",
      "epoch: 1 \n",
      " counter: 354\n",
      "epoch: 1 \n",
      " counter: 355\n",
      "epoch: 1 \n",
      " counter: 356\n",
      "epoch: 1 \n",
      " counter: 357\n",
      "epoch: 1 \n",
      " counter: 358\n",
      "epoch: 1 \n",
      " counter: 359\n",
      "epoch: 1 \n",
      " counter: 360\n",
      "epoch: 1 \n",
      " counter: 361\n",
      "epoch: 1 \n",
      " counter: 362\n",
      "epoch: 1 \n",
      " counter: 363\n",
      "epoch: 1 \n",
      " counter: 364\n",
      "epoch: 1 \n",
      " counter: 365\n",
      "epoch: 1 \n",
      " counter: 366\n",
      "epoch: 1 \n",
      " counter: 367\n",
      "epoch: 1 \n",
      " counter: 368\n",
      "epoch: 1 \n",
      " counter: 369\n",
      "epoch: 1 \n",
      " counter: 370\n",
      "epoch: 1 \n",
      " counter: 371\n",
      "epoch: 1 \n",
      " counter: 372\n",
      "epoch: 1 \n",
      " counter: 373\n",
      "epoch: 1 \n",
      " counter: 374\n",
      "epoch: 1 \n",
      " counter: 375\n",
      "epoch: 1 \n",
      " counter: 376\n",
      "epoch: 1 \n",
      " counter: 377\n",
      "epoch: 1 \n",
      " counter: 378\n",
      "epoch: 1 \n",
      " counter: 379\n",
      "epoch: 1 \n",
      " counter: 380\n",
      "epoch: 1 \n",
      " counter: 381\n",
      "epoch: 1 \n",
      " counter: 382\n",
      "epoch: 1 \n",
      " counter: 383\n",
      "epoch: 1 \n",
      " counter: 384\n",
      "epoch: 1 \n",
      " counter: 385\n",
      "epoch: 1 \n",
      " counter: 386\n",
      "epoch: 1 \n",
      " counter: 387\n",
      "epoch: 1 \n",
      " counter: 388\n",
      "epoch: 1 \n",
      " counter: 389\n",
      "epoch: 1 \n",
      " counter: 390\n",
      "epoch: 1 \n",
      " counter: 391\n",
      "epoch: 1 \n",
      " counter: 392\n",
      "epoch: 1 \n",
      " counter: 393\n",
      "epoch: 1 \n",
      " counter: 394\n",
      "epoch: 1 \n",
      " counter: 395\n"
     ]
    }
   ],
   "source": [
    "epoches = 2\n",
    "# save the variables every n epoch\n",
    "save_every_n = 20\n",
    "\n",
    "model = CharRNN(len(vocab),batch_size=batch_size,num_steps=num_steps,\n",
    "               lstm_size=lstm_size,num_layers=num_layers,\n",
    "               learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epoches):\n",
    "        # train netword\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x,y in get_batches(encoded,batch_size,num_steps):\n",
    "            print('epoch:',e,'\\n','counter:',counter)\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs:x,\n",
    "                   model.targets:y,\n",
    "                   model.keep_prob:keep_prob,\n",
    "                   model.initial_state:new_state}\n",
    "            batch_loss,new_state,_ = sess.run([model.loss,model.final_state,model.optimizer],feed_dict=feed)\n",
    "            end = time.time()\n",
    "            \n",
    "            #control the print lines\n",
    "            if counter % 100 == 0:\n",
    "                print('轮数: {}/{}... '.format(e+1, epoches),\n",
    "                      '训练步数: {}... '.format(counter),\n",
    "                      '训练误差: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            if counter % save_every_n == 0:\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i380_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i40_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i60_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i80_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i100_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i120_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i140_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i160_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i180_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i220_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i240_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i260_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i280_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i300_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i320_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i340_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i360_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i380_l512.ckpt\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds,vocab_size,top_n=5):\n",
    "    '''\n",
    "    从预测结果中选取前top_n个最可能的字符\n",
    "    \n",
    "    preds:预测结果\n",
    "    vocab_size\n",
    "    top_n\n",
    "    '''\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    \n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size,1,p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint,n_samples,lstm_size,vocab_size,prime='The'):\n",
    "    '''\n",
    "    generate new text\n",
    "    \n",
    "    checkpoint: the parameters generated by some epoch\n",
    "    n_sample: the lenght of the text to be generated\n",
    "    lstm_size: the number of hidden units\n",
    "    vocab_size\n",
    "    prime: the start vocabulary of the text\n",
    "    '''\n",
    "    \n",
    "    samples = [c for c in prime]\n",
    "    # sampling=Ture 意味着batch的size=1x1\n",
    "    model = CharRNN(len(vocab),lstm_size=lstm_size,sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        #加载模型，恢复训练\n",
    "        saver.restore(sess,checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1,1))\n",
    "            # input a single char\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs:x,model.keep_prob:1.,\n",
    "                   model.initial_state:new_state}\n",
    "            preds,new_state = sess.run([model.prediction,model.final_state],\n",
    "                                      feed_dict=feed)\n",
    "            \n",
    "        c = pick_top_n(preds,len(vocab))\n",
    "        # add char to samples\n",
    "        samples.append(int_to_vocab[c])\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs:x,model.keep_prob:1,model.initial_state:new_state}\n",
    "            preds,new_state = sess.run([model.prediction,model.final_state],feed_dict=feed)\n",
    "            \n",
    "            c = pick_top_n(preds,len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "            \n",
    "        return ''.join(samples)    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i380_l512.ckpt'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i380_l512.ckpt\n"
     ]
    }
   ],
   "source": [
    "samp = sample(checkpoint,2000,lstm_size,len(vocab),prime='The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ther\\nelse, asd santting hid and wentent of\\nthe cerpuned to the somered, to he saliding of the had hed and the casd torther,\" she had sell wish and at he cald on him sere that to the tho cas out ho warl whot a dover his war of thit some then wat her sald the\\nsaid out the celpase taring hevored his\\nwentere wish wore sat and tour the cous at tom hered and sourt in a derang on hes it he wan worle thate her all som, and wint whit the hind and heving to hem. \"Wher, she\\nwast it of\\nsher the her as time the prise sad ithe she with to mast he was as the pesese fas hemsers, and witl thourtinn thet the wose whot the\\nsind, ther the casse of it an the posed tho carter and his to had hor sithed the sontint\\nwal wat it the she cunsed, hind the persterte ale he maditing on homes an his anded the soling has the wald.\"\\n\\n\"Yet sime to sha chas or the piled to to he somer, the pasting, as the wout tit\\non waser out of thoushed that\\nwat and the sam that thet hom ta her here shand wesh whis ham hard an has hored and that sham te whre he pronte ta the comed the songent was intely heve to has wand whe sat thased to her and thet sit the\\nchad his tond atd this the cousten he mist to\\ntis of the toule tore an his whase to mon wint the\\nchim sard ander ham. The sham and will as has wase and wist the hing the some wist al the werser the hrose sal thas to that, a thame was ofer and the wis onerstinc ot tous ins oftite her as insated, ward the hivers interting and hater was ited the couled and to he sole, ho cult he\\npald, and the cease and wat his, ande the pasising he wasted, and with so cand, the said at his ad with the cile ase whith\\nshat as souled on athen hersed tile he wand ant the sham the wither ther thite hes souted to mene thim an ond tore har he ander ta that the hasd hevined to him had the pored and her sale her whate wat is him of the cundse of the pas somed and his\\nwelle to ham, so sto dite at the\\nshimith him the the candint had wath to hat was he sad anding and the cered, wat in ta presting '"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
